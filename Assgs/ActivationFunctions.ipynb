{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1\n",
    "What is an activation function in the context of artificial neural networks?\n",
    "#### Ans\n",
    "An activation function is a mathematical funciton that is applied to the sum of the products of the weights and the inputs. It is used to bring out the activation level of the neuron.   \n",
    "   It is used to bring non-linearity to the model, thereby modelling complex realationships between the inputs and the outputs. Without non-linear activation functions, the network would only be able to approximate linear functions, severely limiting its representational power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2\n",
    "What are some common types of activation functions used in neural networks?\n",
    "#### Ans\n",
    "Some common types of activation functions are:   \n",
    "* Sigmoid Function : It is represented by the mathematical equation: Sigmoid(z)=1/(1+e^(z)); where z =sigma(xi.wi). It suffers from the Vanishing Gradient problem, because of the small value of its slope.\n",
    "* Hyperbolic Tangent (tanh) Function: It is represented by the mathematical equation: tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z)). Similar to the sigmoid function, the tanh function maps the input to a value between -1 and 1. It is also smooth and non-linear but centered around zero.\n",
    "* Rectified Linear Unit (ReLU): The ReLU activation function sets all negative inputs to zero and keeps positive inputs unchanged. Mathematically, ReLU(z) = max(0, z). ReLU is computationally efficient and addresses the vanishing gradient problem better than sigmoid and tanh functions. However, it suffers from the \"dying ReLU\" problem.\n",
    "* Leaky ReLU: It is an extension of the ReLU function that allows small negative values instead of setting them to zero. Leaky ReLU is defined as LeakyReLU(z) = max(0.01z, z), where 0.01 is a small slope coefficient. This modification helps to mitigate the dying ReLU problem.\n",
    "* Parametric ReLU (PReLU): PReLU is similar to Leaky ReLU but introduces learnable parameters for the slope coefficient instead of using a fixed value. This allows the network to adaptively learn the optimal slope for each neuron during training.\n",
    "* Softmax Function: The softmax function is commonly used in the output layer of a neural network for multi-class classification problems. It normalizes the output values into a probability distribution, where the sum of all class probabilities is equal to 1. Softmax is defined as softmax(z_i) = e^(z_i) / sum(e^(z_j)), where z_i represents the input to the i-th output neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3\n",
    "How do activation functions affect the training process and performance of a neural network?\n",
    "#### Ans\n",
    "Activation functions play a crucial role in the training process and performance of a neural network:\n",
    "\n",
    "* Non-linearity: Activation functions introduce non-linearity, allowing neural networks to model complex relationships between inputs and outputs.\n",
    "* Gradient flow: Activation functions affect the flow of gradients during backpropagation, which is essential for updating the network's weights. \n",
    "* Network convergence: The choice of activation function can impact the convergence speed and stability of the network during training. \n",
    "* Representation power: Different activation functions have varying degrees of representational power. More expressive activation functions can capture intricate patterns and improve the network's ability to learn complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 4\n",
    "How does the sigmoid activation function work? What are its advantages and disadvantages?\n",
    "#### Ans\n",
    "The sigmoid activation function maps the input to a value between 0 and 1. It has the mathematical formula Sigmoid(z) = 1 / (1 + e^(-z)).\n",
    "\n",
    "**Advantages:\n",
    "\n",
    "* It provides a smooth and continuous output, making it easier to compute gradients for training.\n",
    "* It can squash the input to a range between 0 and 1, which can be interpreted as a probability.\n",
    "\n",
    "**Disadvantages:\n",
    "\n",
    "* It suffers from the vanishing gradient problem, making it challenging for deep neural networks to learn effectively.\n",
    "* It is prone to saturation, meaning that very high or low inputs can result in almost zero gradients, leading to slower learning.\n",
    "* The output is not zero-centered, which can cause difficulties in weight updates during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 5\n",
    "Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?\n",
    "#### Ans\n",
    "The ReLU activation function sets all negative inputs to zero and keeps positive inputs unchanged. Mathematically, ReLU(z) = max(0, z).  \n",
    "It differs from sigmoid in the following ways:\n",
    "* It solves the vanishing graient problem in the sigmoid function\n",
    "* It's gradient is a constant unlike sigmoid, which ranges from (0,0.25).\n",
    "* It sets all negative inputs to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 6\n",
    "What are the benefits of using the ReLU activation function over the sigmoid function?\n",
    "#### Ans\n",
    "* Avoids vanishing gradient problem: ReLU does not suffer from the vanishing gradient problem as it does not saturate for positive inputs. This allows for more effective training of deep neural networks.\n",
    "\n",
    "* Faster computation: ReLU is computationally efficient compared to the sigmoid function, as it involves simple thresholding operations without exponential calculations.\n",
    "\n",
    "* Sparse activation: ReLU can create sparsity in the network by setting negative inputs to zero. This can lead to improved generalization.\n",
    "\n",
    "* Avoids saturation: Unlike sigmoid, ReLU does not saturate for large positive inputs, preventing the saturation-induced bottleneck in learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 7\n",
    "Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem.\n",
    "#### Ans\n",
    "Leaky ReLU is an extension of the ReLU activation function that addresses the vanishing gradient problem. It introduces a small slope or leakage for negative inputs, preventing the gradient from becoming zero and allowing for non-zero gradients even for negative inputs. Mathematically, it is represented as LeakyReLU(z) = max(kz, z), where k is a small constant.  \n",
    "By introducing a non-zero gradient for negative inputs, leaky ReLU ensures that the network can continue to learn even in the presence of negative activations. This helps alleviate the vanishing gradient problem, which occurs when gradients become extremely small or zero, hindering the training process in deep neural networks.  \n",
    "The small constant, typically set to a small positive value like 0.01, determines the slope of the function for negative inputs. It is a hyperparameter that can be tuned based on the specific problem and data characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 8\n",
    "What is the purpose of the softmax activation function? When is it commonly used?\n",
    "#### Ans\n",
    "The softmax activation function converts a vector into a probability distribution. The purpose of the softmax function is to provide normalized probabilities, allowing the network to make confident predictions about the class or category to which an input belongs. By producing a probability distribution, it enables the selection of the most likely class or combination of classes based on the network's outputs.   \n",
    "It is commonly used in the output layer for multi-class classification tasks to obtain normalized probabilities for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 9\n",
    "Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?\n",
    "#### Ans\n",
    "The hyperbolic tangent (tanh) activation function is a non-linear function that maps the input to a value between -1 and 1. It is defined as tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z)), where z represents the input.  \n",
    "Compared to the sigmoid function, the tanh function produces outputs that are zero-centered, meaning the average output is closer to zero. This property can facilitate the learning process in certain scenarios. However, like the sigmoid function, the tanh function can also suffer from the vanishing gradient problem for extreme input values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
